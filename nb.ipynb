{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f2d55-88a9-42c5-b566-7b381e86d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2e63f-0e18-4b1e-9183-cc16ee10d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_CHARS = 1024\n",
    "MAX_FEATURES = 3072\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8096218-770c-4217-b043-ec63bb526190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, folder, label):\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        self.paths = []\n",
    "        for filename in os.listdir(folder):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()[:MAX_CHARS]\n",
    "                self.texts.append(text)\n",
    "                self.labels.append(label)\n",
    "                self.paths.append(filepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx], self.paths[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def8515-fd13-48ae-9b87-b1b603506d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "# negative_folder = \"enron/kaminski-nyt\" # llm generated\n",
    "# negative_folder = \"enron/top11-o\" # 11 enron senders\n",
    "negative_folder = \"enron/BEC-2-emails\" # BEC-2 + llm generated + 11 enron senders \n",
    "# negative_folder = \"enron/nyt-alt\" # BEC-2 + 11 enron senders\n",
    "# positive_folder = \"enron/kaminski-v\" # kaminski-v sender\n",
    "# positive_folder = \"enron/kaminski-nyt\"\n",
    "positive_folder = \"enron/stclair-c\" # stclair-c sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d929666-c39a-4ffe-8a79-8d509bb42a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dataset = EmailDataset(negative_folder, 0)\n",
    "pos_dataset = EmailDataset(positive_folder, 1)\n",
    "\n",
    "all_texts = neg_dataset.texts + pos_dataset.texts\n",
    "all_labels = neg_dataset.labels + pos_dataset.labels\n",
    "all_paths = neg_dataset.paths + pos_dataset.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf5034-4e38-4bd9-9113-b1477b25544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Dataset\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, texts, labels, paths):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx], self.paths[idx]\n",
    "\n",
    "dataset = CombinedDataset(all_texts, all_labels, all_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8223fd88-664e-4513-9634-a06c11efa9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random train/test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_texts = [x[0] for x in train_set]\n",
    "train_labels = [x[1] for x in train_set]\n",
    "train_paths = [x[2] for x in train_set]\n",
    "test_texts = [x[0] for x in test_set]\n",
    "test_labels = [x[1] for x in test_set]\n",
    "test_paths = [x[2] for x in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04ceda-a288-4028-95d7-04cbd4a4fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorization\n",
    "vectorizer = CountVectorizer(lowercase=False, stop_words='english', max_features=MAX_FEATURES)\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6cd02-0365-4c81-a188-e7310861057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "model.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d863283b-cdbd-42b0-aa1a-271430326ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "f1 = f1_score(test_labels, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65cd37-dace-4a87-ab95-967c65ceda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793cb201-4a76-47f5-8d41-72d3e741665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify false predictions\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "\n",
    "for true_label, pred_label, file_path in zip(test_labels, y_pred, test_paths):\n",
    "    if true_label == 0 and pred_label == 1:\n",
    "        false_positives.append(file_path)\n",
    "    elif true_label == 1 and pred_label == 0:\n",
    "        false_negatives.append(file_path)\n",
    "\n",
    "print(\"\\nFalse Positives:\")\n",
    "for path in false_positives:\n",
    "    print(f\"  {path}\")\n",
    "\n",
    "print(\"\\nFalse Negatives:\")\n",
    "for path in false_negatives:\n",
    "    print(f\"  {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8d43d7-f990-466b-a94a-526db8a4a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "cm_labels = np.array([1, 0])\n",
    "ConfusionMatrixDisplay.from_predictions(test_labels, y_pred, colorbar=False, labels=cm_labels, cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6467f9-07c5-407e-8d8f-55f7cda45b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Top 10 Most Useful Words in Naive Bayes Classification\n",
    "\n",
    "# Get feature names from the CountVectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get log probabilities of features for each class\n",
    "log_probs = model.feature_log_prob_  # shape: [n_classes, n_features]\n",
    "\n",
    "# Compute difference in log probs between classes (positive - negative)\n",
    "# If class 1 = positive, class 0 = negative\n",
    "log_prob_diff = log_probs[1] - log_probs[0]\n",
    "\n",
    "# Get indices of top 10 most indicative words for positive class\n",
    "top_pos_indices = np.argsort(log_prob_diff)[-10:][::-1]\n",
    "top_neg_indices = np.argsort(log_prob_diff)[:10]\n",
    "\n",
    "# Get top words\n",
    "top_positive_words = [(feature_names[i], log_prob_diff[i]) for i in top_pos_indices]\n",
    "top_negative_words = [(feature_names[i], log_prob_diff[i]) for i in top_neg_indices]\n",
    "\n",
    "print(\"Top 10 Positive Class Words:\")\n",
    "for word, score in top_positive_words:\n",
    "    print(f\"{word:20} {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 Negative Class Words:\")\n",
    "for word, score in top_negative_words:\n",
    "    print(f\"{word:20} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891b2b5-ac8e-4ed5-81e3-ca9a344d6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word names and scores from top_* lists\n",
    "positive_words = [word for word, _ in top_positive_words]\n",
    "positive_scores = [score for _, score in top_positive_words]\n",
    "\n",
    "negative_words = [word for word, _ in top_negative_words]\n",
    "negative_scores = [score for _, score in top_negative_words]\n",
    "\n",
    "# Combine word labels with class tags and absolute scores\n",
    "combined_words = [f\"+ {w}\" for w in positive_words] + [f\"- {w}\" for w in negative_words]\n",
    "combined_scores = [abs(s) for s in positive_scores + negative_scores]\n",
    "\n",
    "# Sort by informativeness (descending)\n",
    "sorted_indices = np.argsort(combined_scores)[::-1]\n",
    "combined_words = [combined_words[i] for i in sorted_indices]\n",
    "combined_scores = [combined_scores[i] for i in sorted_indices]\n",
    "\n",
    "# Grayscale colors\n",
    "gray_colors = plt.cm.Greys(np.linspace(0.4, 0.9, len(combined_scores)))\n",
    "\n",
    "# Plot (taller and narrower)\n",
    "plt.figure(figsize=(8, 8))  # Adjust for two-column fit\n",
    "bars = plt.barh(combined_words[::-1], combined_scores[::-1], color=gray_colors[::-1])\n",
    "plt.xlabel(\"Informativeness (|Log Probability Difference|)\")\n",
    "# plt.title(\"Most Informative Words (Naive Bayes)\", fontsize=11)o\n",
    "\n",
    "# Set font size for the Y-axis labels\n",
    "plt.yticks(fontsize=11)  # Adjust this value as needed\n",
    "\n",
    "# Annotate each bar\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(\n",
    "        width + 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{width:.2f}\",\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        fontsize=8,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e9402-eab7-4f18-88bd-b396b81019b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
